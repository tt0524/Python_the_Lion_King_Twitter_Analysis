{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMPE 256 Summer 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twint\n",
    "# c = twint.Config()\n",
    "# c.Search = \"the lion king\"\n",
    "# c.Store_json = True\n",
    "# c.Output = \"my_twitter_json.json\"\n",
    "# twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('my_twitter_json.json',lines=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to remove duplicated records\n",
    "df.groupby('id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 1157990430451142656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 1157992220995117056]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 1157991907294736385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove duplicated row by id\n",
    "print(\"Before remove duplicates: \" + str(df.shape))\n",
    "df = df.drop_duplicates(subset = 'id', keep='first')\n",
    "print(\"After remove duplicates: \" + str(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop less important columns\n",
    "df = df.drop(columns=['retweet','cashtags','video','retweets_count','replies_count','photos'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tweet[0])\n",
    "print(df.urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(df.tweet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notice there's usually a hyperlink at the end of the tweet, to analyze the text more precisely, would remove\n",
    "## these website url links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.tweet[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    r\"\"\" remove url links in text\n",
    "    \n",
    "    Args:\n",
    "        text: string\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"     \n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #remove urls: http://XXXX\n",
    "    text = re.sub(r\"\\S+.twitter.com\\/\\S+\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_no_url'] = df['tweet'].map(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_no_url'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(df['tweet_no_url'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['just_date'] = df['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_count = df.groupby('just_date').agg({'id': np.size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "ax = date_count.plot(kind='bar',figsize=(15,10),legend=False,color='c')\n",
    "ax.set_title(\"Tweets Daily Frequency\")\n",
    "ax.set_xlabel(\"Date\",fontsize=12)\n",
    "ax.set_ylabel(\"# of Tweets\",fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All the records are from PDT Timezone\n",
    "## Will drop timezone \n",
    "df.groupby('timezone').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['timezone'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.time[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hour(string_time):\n",
    "    r\"\"\" extract hour from string time: '09:01:03' -> 09\n",
    "    \n",
    "    Args:\n",
    "        string_time: '09:01:03'\n",
    "    \n",
    "    Returns:\n",
    "        Hour\n",
    "    \"\"\"      \n",
    "    hour = int(string_time.split(\":\")[0])\n",
    "    return hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df.time.map(extract_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hour[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_count = df.groupby('hour').agg({'id': np.size})\n",
    "plt.style.use('ggplot')\n",
    "ax = hourly_count.plot(kind='bar',figsize=(15,10),legend=False,color='c')\n",
    "ax.set_title(\"Tweets Hourly Frequency\")\n",
    "ax.set_xlabel(\"Hour\",fontsize=12)\n",
    "ax.set_ylabel(\"# of Tweets\",fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all tweets into one text\n",
    "all_text = ''\n",
    "for x in df.tweet_no_url:\n",
    "    all_text += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=40).generate(all_text)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "more_stops = ['lion','king','the',\"didn’t\",'lionking','twitter','http','https','post','posts','movie','lion-king']\n",
    "stop = stop + more_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.tweet_no_url[2]\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "more_stops = set(['lion','king','the',\"didn’t\",'lionking','thelionking',\\\n",
    "                  'twitter','http','https','post','posts','movie','lion-king',\\\n",
    "             \"'s\",\"via\",\"go\",\"gonna\",\"wanna\",\"going\",\"got\"])\n",
    "MY_STOP = stop.union(more_stops)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    r\"\"\" Preprocess text data\n",
    "    \n",
    "    Args:\n",
    "        text: text list\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed text list\n",
    "    \"\"\"    \n",
    "    chars = set(['’','1','2','3','4','5','6','7','8','9','0','!',',','_','.','#','...','—','-','@','..',])\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\S+.twitter.com\\/\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #remove urls: http://XXXX\n",
    "    texts = nltk.word_tokenize(text)\n",
    "    # remove stop words\n",
    "    result = [x for x in texts if x not in MY_STOP]\n",
    "    result = [x for x in result if x not in chars]\n",
    "    # extract stem word using nltk package\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tweet[9])\n",
    "print(preprocess_text(df.tweet[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_clean'] = df['tweet'].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = ''\n",
    "for t in df.tweet_clean:\n",
    "    clean_text += t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?str.replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the mask image\n",
    "lion_mask = np.array(Image.open(\"black_lion.png\"))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", mask=lion_mask)\n",
    "# generate word cloud\n",
    "wc.generate(clean_text)\n",
    "# store to file\n",
    "wc.to_file(\"lion.png\")\n",
    "# show\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(lion_mask, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = df.shape[0]\n",
    "n_cols = df.shape[1]\n",
    "print(n_rows)\n",
    "print(n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(n_rows):\n",
    "    if df.tweet_clean[i].find('twitter') > 0:\n",
    "        break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.tweet_clean[73]\n",
    "print(text)\n",
    "## Need to remove short link pic.twitter.com/xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.tweet_clean[9]\n",
    "print(text)\n",
    "text = re.sub(r\"\\S+.twitter.com\\/\\S+\", \"\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hashtags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to extract all the hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_all = []\n",
    "for h in df.hashtags:\n",
    "    if len(h) > 0:\n",
    "        hashtags_all += h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = df.hashtags[19]\n",
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = df.hashtags[18]\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h1 + h2\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(hashtags_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_freq = c.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_freq_name = [x[0] for x in hashtag_freq[::-1]]\n",
    "hashtag_freq_freq = [x[1] for x in hashtag_freq[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.barh(hashtag_freq_name, hashtag_freq_freq,color='c')\n",
    "plt.title('hashtag frequeency(top 30)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtag_freq_name = [x[0] for x in hashtag_freq[::-1]][:28]\n",
    "hashtag_freq_freq = [x[1] for x in hashtag_freq[::-1]][:28]\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.barh(hashtag_freq_name, hashtag_freq_freq,color='c')\n",
    "plt.title('hashtag frequeency(top 30)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet_clean[:90000].to_csv(\"only_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates:\n",
    "    print(date)\n",
    "    df_day = df[df.date == date].tweet\n",
    "    file_name = \"tweet_\" + str(date)[:10] + \".csv\"\n",
    "    df_day.reset_index().tweet.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.reset_index().tweet.to_csv(\"t.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE Google NLP APT to get Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "import six\n",
    "\n",
    "\n",
    "def sample_analyze_sentiment(content):\n",
    "    \n",
    "    CLIENT = language_v1.LanguageServiceClient()\n",
    "    \n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    document = {'type': type_, 'content': content}\n",
    "\n",
    "    try:\n",
    "        response = CLIENT.analyze_sentiment(document)\n",
    "        sentiment = response.document_sentiment\n",
    "        result = sentiment.score\n",
    "    except:\n",
    "        result = 10\n",
    "    return result\n",
    "    # print('Magnitude: {}'.format(sentiment.magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = 'Hello, world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_analyze_sentiment(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['tweet_no_url'].map(sample_analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client.from_service_account_json(\"/Users/xajin/Downloads/google_cred.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = list(storage_client.list_buckets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet_no_url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_analyze_sentiment(df.tweet_no_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = {}\n",
    "for i in range(df.shape[0]):\n",
    "    t_id = df.id[i]\n",
    "    text = df.tweet_no_url[i]\n",
    "    score = sample_analyze_sentiment(text)\n",
    "    sentiment_scores[t_id] = score\n",
    "    if i in (100000, 200000, 300000, 400000, 500000):\n",
    "        print(\"%d texts analyzed\" &i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sentiment_scores.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [x for x in scores if x != 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.id == 1157984313473814528].tweet)\n",
    "print(sentiment_scores[1157984313473814528])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec for Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = []\n",
    "for text in df.tweet_clean:\n",
    "    train_text_list.append(text.split(' '))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        sentences,\n",
    "    from gensim.test.utils import common_texts, get_tmpfile\n",
    "        window=10,\n",
    "        min_count=10,\n",
    "        alpha=0.02,\n",
    "        workers=10)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(['thelionking'],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = model.wv.most_similar(['lionking'],topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = [x for x in my_list if x[0].find('lion') < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakunamatata_n = model.wv.most_similar(['hakunamatata'],topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
